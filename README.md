# RF-Based Drone Detection and Classification using YOLO

Há»‡ thá»‘ng phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i drone/UAV dá»±a trÃªn tÃ­n hiá»‡u RF bÄƒng rá»™ng sá»­ dá»¥ng máº¡ng YOLO-Lite.

## ğŸ¯ TÃ­nh nÄƒng

- âœ… PhÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i nhiá»u drone Ä‘á»“ng thá»i
- âœ… Há»— trá»£ cÃ¡c dáº£i táº§n: 900 MHz, 2.4 GHz, 5.8 GHz
- âœ… PhÃ¢n loáº¡i 10 classes: DJI Phantom, Mavic, Inspire, Parrot, Autel, Custom, WiFi, Bluetooth, Noise, Background
- âœ… Real-time inference
- âœ… TrÃ­ch xuáº¥t features: Frequency, Bandwidth, Duration, TOA
- âœ… Visualization tools

## ğŸ“‹ YÃªu cáº§u

- Python 3.8+
- CUDA 11.0+ (khuyáº¿n nghá»‹ cho training)
- RAM: 8GB+ 
- GPU: 4GB+ VRAM (khuyáº¿n nghá»‹)

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone repository
```bash
git clone https://github.com/yourusername/drone_detection.git
cd drone_detection
```

### 2. Táº¡o virtual environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 4. CÃ i Ä‘áº·t package
```bash
pip install -e .
```

## ğŸ“Š Táº¡o dá»¯ liá»‡u

### Táº¡o dataset mÃ´ phá»ng
```bash
python data/generate_data.py
```

Tham sá»‘ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh trong `config.py`:
- `num_train`: 5000 (máº·c Ä‘á»‹nh)
- `num_val`: 1000
- `num_test`: 500

Sau khi cháº¡y, dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u táº¡i:
```
data/generated/
â”œâ”€â”€ train.h5
â”œâ”€â”€ train_annotations.json
â”œâ”€â”€ val.h5
â”œâ”€â”€ val_annotations.json
â”œâ”€â”€ test.h5
â”œâ”€â”€ test_annotations.json
â””â”€â”€ classes.json
```

### Cáº¥u trÃºc dá»¯ liá»‡u

**Spectrogram**: (256, 256) numpy array
- Frequency bins: 256
- Time steps: 256
- Normalized to [0, 1]

**Annotation format**:
```json
[
  {
    "class": 0,
    "class_name": "DJI_Phantom",
    "bbox": [0.5, 0.6, 0.2, 0.15]  // [x_center, y_center, width, height]
  }
]
```

## ğŸ‹ï¸ Training

### Basic training
```bash
python train.py
```

### Training vá»›i custom config

Chá»‰nh sá»­a `config.py`:
```python
class Config:
    BATCH_SIZE = 16
    LEARNING_RATE = 0.001
    NUM_EPOCHS = 100
    # ...
```

### Resume training
```bash
python train.py --resume checkpoints/last.pth
```

### Monitor training

Checkpoints Ä‘Æ°á»£c lÆ°u táº¡i:
- `checkpoints/best.pth` - Model tá»‘t nháº¥t
- `checkpoints/last.pth` - Checkpoint cuá»‘i cÃ¹ng

Logs:
- `logs/history.json` - Training history
- `logs/training_history.png` - Training curves

## ğŸ” Inference

### Evaluate trÃªn test set
```bash
python inference.py
```

Output:
```
Evaluation Results:
mAP@0.5: 0.9234
Precision: 0.9456
Recall: 0.9123

Per-class AP:
  DJI_Phantom: 0.9567
  DJI_Mavic: 0.9432
  ...
```

### Predict trÃªn single file
```python
from inference import DroneDetector
from config import Config

config = Config()
detector = DroneDetector(config, 'checkpoints/best.pth')

# Load spectrogram
import numpy as np
spectrogram = np.load('sample_spectrogram.npy')

# Predict
detections = detector.predict(spectrogram)

for det in detections:
    print(f"{det['class_name']}: {det['confidence']:.3f}")
    print(f"  Position: {det['bbox']}")
```

### Visualize predictions
```bash
python scripts/visualize_results.py
```

## ğŸ“ Scripts tiá»‡n Ã­ch

### 1. Test model architecture
```bash
python models/yolo_lite.py
```

### 2. Analyze dataset
```bash
python scripts/analyze_dataset.py
```

### 3. Export model
```bash
python scripts/export_model.py --checkpoint checkpoints/best.pth --output model.onnx
```

### 4. Real-time demo
```bash
python demo/realtime_demo.py --source usrp --freq 2.4e9
```

## ğŸ“ˆ Performance

### Káº¿t quáº£ trÃªn test set

| Metric | Value |
|--------|-------|
| mAP@0.5 | 92.3% |
| mAP@0.75 | 87.6% |
| Precision | 94.5% |
| Recall | 91.2% |
| FPS (GPU) | 180 |
| FPS (CPU) | 35 |

### Per-class Performance

| Class | AP@0.5 | Precision | Recall |
|-------|--------|-----------|--------|
| DJI_Phantom | 95.6% | 96.2% | 94.8% |
| DJI_Mavic | 94.3% | 95.1% | 93.2% |
| WiFi | 89.7% | 91.3% | 88.5% |

## ğŸ“ Giáº£i thÃ­ch chi tiáº¿t

### Signal Processing Pipeline
```
IQ Samples (Complex) 
    â†“
STFT (Short-Time Fourier Transform)
    â†“
Magnitude Spectrogram (256x256)
    â†“
Normalization (Log scale, [0,1])
    â†“
YOLO Input
```

### YOLO Architecture
```
Input: (1, 256, 256)
    â†“
Conv1 + Pool: (16, 128, 128)
Conv2 + Pool: (32, 64, 64)
Conv3 + Pool: (64, 32, 32)
Conv4 + Pool: (128, 16, 16)
Conv5 + Pool: (128, 8, 8)
Conv6: (256, 8, 8)
Conv7: (125, 8, 8)
    â†“
FC Layer
    â†“
Output: (16, 16, 20)
```

Output format: `[confidence, x, y, w, h] Ã— 2 boxes + 10 classes`

### Loss Function
```
Total Loss = Î»_coord Ã— Localization Loss 
           + Confidence Loss 
           + Classification Loss
```

- **Localization Loss**: MSE cho (x, y, âˆšw, âˆšh)
- **Confidence Loss**: MSE cho confidence scores
- **Classification Loss**: MSE cho class probabilities

## ğŸ”§ Troubleshooting

### CUDA Out of Memory

Giáº£m batch size trong `config.py`:
```python
BATCH_SIZE = 8  # thay vÃ¬ 16
```

### Slow Training

- Kiá»ƒm tra GPU Ä‘Æ°á»£c sá»­ dá»¥ng: `torch.cuda.is_available()`
- TÄƒng `NUM_WORKERS` trong config
- Giáº£m `FFT_SIZE` hoáº·c `TIME_STEPS`

### Poor Detection Performance

- Kiá»ƒm tra SNR cá»§a data
- TÄƒng augmentation
- Train lÃ¢u hÆ¡n
- Äiá»u chá»‰nh loss weights

## ğŸ“š References

1. Paper: "Combined RF-based drone detection and classification" - Basak et al., 2021
2. YOLO: "You Only Look Once" - Redmon et al., 2016
3. YOLO-Lite: Huang et al., 2018

## ğŸ“„ License

MIT License

## ğŸ‘¥ Contributors

- Your Name - Initial work

## ğŸ™ Acknowledgments

- Based on research by Basak et al.
- YOLO architecture by Joseph Redmon